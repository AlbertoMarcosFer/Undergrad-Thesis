---
title: "clean_code_ajdamico"
author: "Alberto Marcos Fernandez"
date: "2024-05-08"
output: html_document
---

This document can be used to check the statistical computations made to check the first hypotheses formulated in the paper. It adapts Anthony Damico's (2014) formulas for dealing with multiple implicate data (check <https://github.com/DjalmaPessoa/usgsd/blob/master/Survey%20of%20Consumer%20Finances/analysis%20examples.R>).

As with the case for the downloading document, you should also define your working directory below inside setwd(). Please, make sure to use forward slashes (i.e., /) instead of backward slashes (i.e., \\). You should also select the edition of the SCF that you want to analyze inside load(). The format of the datasets' names is "scfYYYY.rda", where YYYY are the four digits of the SCF dataset edition. Changing these four digits for any of the editions downloaded (1998, 2001, 2004, 2007, 2010, 2013, 2016, 2019, 2022) loads the corresponding edition. For example, load("scf2022.rda") would load the 2022 edition.

```{r}
setwd( "C:/Users/amarc/OneDrive/Escritorio/SCF" )
load( "scf2022.rda" )

library(mitools)
library(survey)
library(downloader)
library(foreign)
library(DescTools)
library(dplyr)
library(tidyr)
library(ggplot2)
library(car)
library(pander)

interest_vars <- function(df){
  df$income_percentile <- cut(df$income, unique(quantile(df$income, seq(0, 1, 0.01))), labels = FALSE)
  df$networth_percentile <- cut(df$networth, unique(quantile(df$networth, seq(0, 1, 0.01))), labels = FALSE)
  df$debt2inc <- ifelse(df$debt2inc==0, 0.1, df$debt2inc)
  df$debt2inc <- cut(df$debt2inc, unique(quantile(df$debt2inc, seq(0, 1, 0.333))), labels = FALSE)
  df$levratio <- ifelse(df$levratio==0, 0.1, df$levratio)
  df$levratio <- cut(df$levratio, unique(quantile(df$levratio, seq(0, 1, 0.333))), labels = FALSE)
  df$dummy_deq <- ifelse(df$deq>0, 1, 0)
  df$dummy_deq <- ifelse(df$htrad==1, 1, df$dummy_deq)
  df$lowest_quart <- ifelse(df$income<(median(df$income)/2), 1, 0)

  factor_vars <- c("agecl", "debt2inc", "dummy_deq", "famstruct", "finlit", "hhsex", "housecl", "hstocks", "htrad", "icall", "idont", "ifinplan",  "ifinpro", "ifriendwork", "iinternet",  "imagznews", "imailadtv", "internet", "iother", "iself", "ishopgrdl", "ishopmodr", "ishopnone", "levratio", "lowest_quart", "occat1", "race", "savres1", "nofinrisk")
  df[factor_vars] <- lapply(df[,factor_vars], factor)
  cbind(df, df$dummy_deq, df$income_percentile, df$debt2inc, df$levratio, df$networth_percentile, df$lowest_quart)
  
#I have defined below the subset of variables and observations to keep. To perform the model on the individuals that fall in the lowest quartile you should run the subset as follows instead of the one below:
# subset(df, df$savres1==0 & df$lowest_quart==1, select = c(agecl, debt2inc, dummy_deq, educ, famstruct, finlit, hhsex, housecl, hstocks, htrad, icall, idont, ifinplan, ifinpro, ifriendwork, iinternet, imagznews, imailadtv, income, income_percentile, internet, iother, iself, ishopgrdl, ishopmodr, ishopnone, levratio, lowest_quart, nstocks, ntrad, networth_percentile, nwcat, nwpctlecat, occat1, race, nofinrisk, wgt, y1, yy1))
  subset(df, df$savres1==0 & df$lowest_quart==1, select = c(agecl, debt2inc, dummy_deq, educ, famstruct, finlit, hhsex, housecl, hstocks, htrad, icall, idont, ifinplan, ifinpro, ifriendwork, iinternet, imagznews, imailadtv, income, income_percentile, internet, iother, iself, ishopgrdl, ishopmodr, ishopnone, levratio, lowest_quart, nstocks, ntrad, networth_percentile, nwcat, nwpctlecat, occat1, race, nofinrisk, wgt, y1, yy1))
}

#I have defined the implicates according to the variables of interest of the model
imp1 <- interest_vars(imp1)
imp2 <- interest_vars(imp2)
imp3 <- interest_vars(imp3)
imp4 <- interest_vars(imp4)
imp5 <- interest_vars(imp5)

#Since it is necessary to have the same number of observations to compute the MIresponse() (our model), I have selected only the observations (yy1) that can save (df$savres1==0) across all five implicates. The process below overlaps the observations and selects only the ones that can save across all implicates and does the same for the weight file (to preserve relative weights in the regression).
common_obs <- Reduce(intersect, list(imp1$yy1, imp2$yy1, imp3$yy1, imp4$yy1, imp5$yy1, rw$yy1))
imp1 <- imp1[imp1$yy1 %in% common_obs, ]
imp2 <- imp2[imp2$yy1 %in% common_obs, ]
imp3 <- imp3[imp3$yy1 %in% common_obs, ]
imp4 <- imp4[imp4$yy1 %in% common_obs, ]
imp5 <- imp5[imp5$yy1 %in% common_obs, ]
rw <- rw[rw$yy1 %in% common_obs, ]

# clear up RAM
gc()

# end of memory conservation step #


# turn off scientific notation in most output
options( scipen = 20 )

scf.svyttest<-function(formula, design ,...){

	# the MIcombine function runs differently than a normal svyglm() call
	m <- eval(bquote(MIcombine( with( design , svyglm(formula,family=gaussian()))) ) )

	rval<-list(statistic=coef(m)[2]/SE(m)[2],
			   parameter=m$df[2],		
			   estimate=coef(m)[2],
			   null.value=0,
			   alternative="two.sided",
			   method="Design-based t-test",
			   data.name=deparse(formula))
			   
	rval$p.value <- ( 1 - pf( ( rval$statistic )^2 , 1 , m$df[2] ) )

	names(rval$statistic)<-"t"
	names(rval$parameter)<-"df"
	names(rval$estimate)<-"difference in mean"
	names(rval$null.value)<-"difference in mean"
	class(rval)<-"htest"

	return(rval)
  
}


# MIcombine() variant (code from the `mitools` package) that only uses
# the sampling variance from the *first* imputation instead of averaging all five
scf.MIcombine <-
	function (results, variances, call = sys.call(), df.complete = Inf, ...) {
		m <- length(results)
		oldcall <- attr(results, "call")
		if (missing(variances)) {
			variances <- suppressWarnings(lapply(results, vcov))
			results <- lapply(results, coef)
		}
		vbar <- variances[[1]]
		cbar <- results[[1]]
		for (i in 2:m) {
			cbar <- cbar + results[[i]]
			# MODIFICATION:
			# vbar <- vbar + variances[[i]]
		}
		cbar <- cbar/m
		# MODIFICATION:
		# vbar <- vbar/m
		evar <- var(do.call("rbind", results))
		r <- (1 + 1/m) * evar/vbar
		df <- (m - 1) * (1 + 1/r)^2
		if (is.matrix(df)) df <- diag(df)
		if (is.finite(df.complete)) {
			dfobs <- ((df.complete + 1)/(df.complete + 3)) * df.complete * 
			vbar/(vbar + evar)
			if (is.matrix(dfobs)) dfobs <- diag(dfobs)
			df <- 1/(1/dfobs + 1/df)
		}
		if (is.matrix(r)) r <- diag(r)
		rval <- list(coefficients = cbar, variance = vbar + evar * 
		(m + 1)/m, call = c(oldcall, call), nimp = m, df = df, 
		missinfo = (r + 2/(df + 3))/(r + 1))
		class(rval) <- "MIresult"
		rval
	}


# construct an imputed replicate-weighted survey design object
# build a new replicate-weighted survey design object,
# but unlike most replicate-weighted designs, this object includes the
# five multiply-imputed data tables - imp1 through imp5
scf.design <- 
	svrepdesign( 
		
		# use the main weight within each of the imp# objects
		weights = ~wgt , 
		
		# use the 999 replicate weights stored in the separate replicate weights file
		repweights = rw[ , -1 ] , 
		
		# read the data directly from the five implicates
		data = imputationList( list( imp1 , imp2 , imp3 , imp4 , imp5 ) ) , 

		scale = 1 ,

		rscales = rep( 1 / 998 , 999 ) ,

		# use the mean of the replicate statistics as the center
		# when calculating the variance, as opposed to the main weight's statistic
		mse = TRUE ,
		
		type = "other" ,

		combined.weights = TRUE
	)
```

The code below computes the model respecting the SCF design requirements. For an explanation of the variables' meaning check: <https://sda.berkeley.edu/sdaweb/docs/scfcomb2022/DOC/hcbkx01.htm>

```{r}
model_h1 <- MIcombine(
  with(scf.design, 
  svyglm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, family = quasibinomial)))

tStat <- model_h1$coefficients/sqrt(diag(model_h1$variance))
p_values <- round(2*pt(-abs(tStat),df=model_h1$df), 16)
significance <- add.significance.stars(p_values, cutoffs = c(0.05, 0.01, 0.001))
complete_2022_h1 <- cbind(summary(model_h1), tStat, p_values, significance)
complete_2022_h1
```

You can save the model in the folder you wish in order to avoid repeating the same computations:

```{r}
saveRDS(complete_2022_h1, file = "C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2022_h1.rds")
```

The code below checks if the data structure is adequate in order to comply with the assumptions of the logistic model. These characteristics are checked on each of the five implicates separately instead of the combined one as the functions required to do so do not work on MIresult objects (like the model estimated above). The assumptions taken when estimating a logit model are:

1.  Independence of observations: this is guaranteed by the design of the Survey of Consumer Finances. Not only are observations independent of each other but they are also weighted in accordance with the general US population in order not to get biased estimations. This assumption requires that we take into account the design of the SCF (its multiple implicates per observation), which we do above (see *scf.design*).

2.  Linearity in the logit for continuous variables. In addition to *income_percentile*, I have decided to treat *educ* as continuous since it is divided into many ordinal stages that hold a constant marginal relationship with the logarithm of the odds ratio of the dependent variable (*dummy_deq*: stock market participation).

```{r}
linear <- function(df){
  df <- na.omit(df)
  logistic_model <- glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = df, family = binomial)
  probabilities <- predict(logistic_model, type = "response")
  predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
  mydata <- subset(df, select=-c(income, nstocks, ntrad, nwcat, nwpctlecat, y1, yy1, wgt)) %>%
    dplyr::select_if(is.numeric) 
  predictors <- colnames(df)
  mydata <- mydata %>%
    mutate(logit = log(probabilities/(1-probabilities))) %>%
    gather(key = "predictors", value = "predictor.value", -logit)

  mydata = subset(mydata, mydata$logit>-10)

  mydata = subset(mydata, mydata$logit<5)
  
  ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "gam") + 
  theme(aspect.ratio = 1) + 
  facet_wrap(~predictors, scales = "free_y")
}

linear(imp1)
linear(imp2)
linear(imp3)
linear(imp4)
linear(imp5)
```

3.  Lack of strongly influential outliers. Cook's distances over 1 represent influential outliers (R. Dennis Cook, 1982). Therefore, SCF datasets do not present influential outliers.

```{r}
cook_distance <- function(df){
  logistic_model <- glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = df, family = binomial)
  cooksd <- cooks.distance(logistic_model)
  cooksd_out <- subset(cooksd, cooksd<(3*mean(cooksd)))
  plot(cooksd, pch = 20, main = "Cook's Distance Plot", ylab = "Cook's Distance", xlab = "Observation")
}

cook_distance(imp1)
cook_distance(imp2)
cook_distance(imp3)
cook_distance(imp4)
cook_distance(imp5)
```

4.  Absence of multicollinearity. VIF tests allow us to check for multicollinearity among variables. Values in the 0-5 range indicate low multicollinearity with other variables. The 5-10 range indicates moderate multicollinearity and values over 10 indicate high multicollinearity with other variables. The only concerning case in our dataset is the case of the *networth_percentile* variable, which usually lies just above the 5 threshold. However, since it is relatively low in the 5-10 range and the variable is highly significant at all levels considered, I have deliberately decided to keep it in spite of its moderately high multicollinearity among other variables. For the subset of observations in the lowest quartile, *age* and *famstruct* also present high VIF values. Since they are mere control variables, I have decided to keep them.

```{r}
vif_check <- function(df){
  logistic_model <- glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = df, family = binomial)
  vif_values <- car::vif(logistic_model)
  print(vif_values)
}

vif_check(imp1)
vif_check(imp2)
vif_check(imp3)
vif_check(imp4)
vif_check(imp5)
```

I have also computed a diagnostic test using the Pseudo-R-squared by McFadden. Values between 0.2 and 0.4 are acceptable. Values above 0.4 indicate great goodness of fit (McFadden, 1977).

```{r}
PseudoR2(glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp1, family = binomial), which = "McFadden")
PseudoR2(glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp2, family = binomial), which = "McFadden")
PseudoR2(glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp3, family = binomial), which = "McFadden")
PseudoR2(glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp4, family = binomial), which = "McFadden")
PseudoR2(glm(dummy_deq ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + iinternet + imagznews + imailadtv + income_percentile + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp5, family = binomial), which = "McFadden")
```

If you chose to download the models and want to open them (selecting your chosen location of the RDS file) you can do so with the code below:

```{r}
lowest_quart_1998_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_1998_h1.rds")
lowest_quart_2001_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2001_h1.rds")
lowest_quart_2004_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2004_h1.rds")
lowest_quart_2007_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2007_h1.rds")
lowest_quart_2010_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2010_h1.rds")
lowest_quart_2013_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2013_h1.rds")
lowest_quart_2016_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2016_h1.rds")
lowest_quart_2019_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2019_h1.rds")
lowest_quart_2022_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/low_income_models/lowest_quart_2022_h1.rds")

complete_1998_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_1998_h1.rds")
complete_2001_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2001_h1.rds")
complete_2004_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2004_h1.rds")
complete_2007_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2007_h1.rds")
complete_2010_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2010_h1.rds")
complete_2013_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2013_h1.rds")
complete_2016_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2016_h1.rds")
complete_2019_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2019_h1.rds")
complete_2022_h1 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h1/models_all_dataset/complete_2022_h1.rds")
```

The objects created below are bound into a data frame in order to compute the evolution of the coefficients and their statistical significance along the years:

```{r}
internet_complete = rbind(exp(complete_1998_h1["internet", 1]),
exp(complete_2001_h1["internet", 1]),
exp(complete_2004_h1["internet", 1]),
exp(complete_2007_h1["internet", 1]),
exp(complete_2010_h1["internet", 1]),
exp(complete_2013_h1["internet", 1]),
exp(complete_2016_h1["internet", 1]),
exp(complete_2019_h1["internet", 1]),
exp(complete_2022_h1["internet", 1]))

significance_internet_complete= rbind((complete_1998_h1["internet", 8]),
(complete_2001_h1["internet", 8]),
(complete_2004_h1["internet", 8]),
(complete_2007_h1["internet", 8]),
(complete_2010_h1["internet", 8]),
(complete_2013_h1["internet", 8]),
(complete_2016_h1["internet", 8]),
(complete_2019_h1["internet", 8]),
(complete_2022_h1["internet", 8]))

iinternet_complete = rbind(exp(complete_1998_h1["iinternet", 1]),
exp(complete_2001_h1["iinternet", 1]),
exp(complete_2004_h1["iinternet", 1]),
exp(complete_2007_h1["iinternet", 1]),
exp(complete_2010_h1["iinternet", 1]),
exp(complete_2013_h1["iinternet", 1]),
exp(complete_2016_h1["iinternet", 1]),
exp(complete_2019_h1["iinternet", 1]),
exp(complete_2022_h1["iinternet", 1]))

significance_iinternet_complete = rbind((complete_1998_h1["iinternet", 8]),
(complete_2001_h1["iinternet", 8]),
(complete_2004_h1["iinternet", 8]),
(complete_2007_h1["iinternet", 8]),
(complete_2010_h1["iinternet", 8]),
(complete_2013_h1["iinternet", 8]),
(complete_2016_h1["iinternet", 8]),
(complete_2019_h1["iinternet", 8]),
(complete_2022_h1["iinternet", 8]))

internet_lowest_quart = rbind(exp(lowest_quart_1998_h1["internet", 1]),
exp(lowest_quart_2001_h1["internet", 1]),
exp(lowest_quart_2004_h1["internet", 1]),
exp(lowest_quart_2007_h1["internet", 1]),
exp(lowest_quart_2010_h1["internet", 1]),
exp(lowest_quart_2013_h1["internet", 1]),
exp(lowest_quart_2016_h1["internet", 1]),
exp(lowest_quart_2019_h1["internet", 1]),
exp(lowest_quart_2022_h1["internet", 1]))

significance_internet_lowest_quart = rbind((lowest_quart_1998_h1["internet", 8]),
(lowest_quart_2001_h1["internet", 8]),
(lowest_quart_2004_h1["internet", 8]),
(lowest_quart_2007_h1["internet", 8]),
(lowest_quart_2010_h1["internet", 8]),
(lowest_quart_2013_h1["internet", 8]),
(lowest_quart_2016_h1["internet", 8]),
(lowest_quart_2019_h1["internet", 8]),
(lowest_quart_2022_h1["internet", 8]))

iinternet_lowest_quart = rbind(exp(lowest_quart_1998_h1["iinternet", 1]),
exp(lowest_quart_2001_h1["iinternet", 1]),
exp(lowest_quart_2004_h1["iinternet", 1]),
exp(lowest_quart_2007_h1["iinternet", 1]),
exp(lowest_quart_2010_h1["iinternet", 1]),
exp(lowest_quart_2013_h1["iinternet", 1]),
exp(lowest_quart_2016_h1["iinternet", 1]),
exp(lowest_quart_2019_h1["iinternet", 1]),
exp(lowest_quart_2022_h1["iinternet", 1]))

significance_iinternet_lowest_quart = rbind((lowest_quart_1998_h1["iinternet", 8]),
(lowest_quart_2001_h1["iinternet", 8]),
(lowest_quart_2004_h1["iinternet", 8]),
(lowest_quart_2007_h1["iinternet", 8]),
(lowest_quart_2010_h1["iinternet", 8]),
(lowest_quart_2013_h1["iinternet", 8]),
(lowest_quart_2016_h1["iinternet", 8]),
(lowest_quart_2019_h1["iinternet", 8]),
(lowest_quart_2022_h1["iinternet", 8]))

coef_trends <- data.frame(Time = c(1998, 2001, 2004, 2007, 2010, 2013, 2016, 2019, 2022), "internet_complete" = internet_complete, "significance_internet_complete" = significance_internet_complete, "iinternet_complete" = iinternet_complete, "significance_iinternet_complete"= significance_iinternet_complete, "internet_lowest_quart" = internet_lowest_quart, "significance_internet_lowest_quart" = significance_internet_lowest_quart, "iinternet_lowest_quart" = iinternet_lowest_quart, "significance_iinternet_lowest_quart" = significance_iinternet_lowest_quart)
```

We create the plots that allow us to see the evolution of *internet* and *iinternet* coefficients along the years:

```{r}
ggplot() +
  geom_line(data= coef_trends, aes(y = internet_complete, x = Time, color="Complete SCF dataset") ) +
  geom_text(data=coef_trends, aes(x=Time, y=internet_complete, label=significance_internet_complete), vjust = 0) +
  geom_line(data= coef_trends, aes(y= internet_lowest_quart, x = Time, color = "Lowest quartile")) +
  geom_text(data=coef_trends, aes(x=Time, y=internet_lowest_quart, label=significance_internet_lowest_quart), vjust = 0) +
  scale_color_manual(name = "Case of study", values = c("Complete SCF dataset" = "darkblue", "Lowest quartile" = "red")) +
  labs(title = "Access to Online Financial Institutions",
       x = "Year",
       y = "Odds Ratio")
  
ggplot(coef_trends, aes(x = Time)) +
  geom_line(data= coef_trends, aes(y = iinternet_complete, x = Time, color="Complete SCF dataset") ) +
  geom_text(data=coef_trends, aes(x=Time, y=iinternet_complete, label=significance_iinternet_complete), vjust = 0) +
  geom_line(data= coef_trends, aes(y= iinternet_lowest_quart, x = Time, color = "Lowest quartile")) +
  geom_text(data=coef_trends, aes(x=Time, y=iinternet_lowest_quart, label=significance_iinternet_lowest_quart), vjust = 0) +
  scale_color_manual(name = "Case of study", values = c("Complete SCF dataset" = "darkblue", "Lowest quartile" = "red")) +
  labs(title = "Access to Financial Information Online",
       x = "Year",
       y = "Odds Ratio")
```
