---
title: "Hypothesis_2"
author: "Alberto Marcos Fernandez"
date: "2024-05-10"
output: html_document
---

This document can be used to check the statistical computations made to check the first hypotheses formulated in the paper. It adapts Anthony Damico's (2014) formulas for dealing with multiple implicate data (check <https://github.com/DjalmaPessoa/usgsd/blob/master/Survey%20of%20Consumer%20Finances/analysis%20examples.R>).

As with the case for the downloading document, you should also define your working directory below inside setwd(). Please, make sure to use forward slashes (i.e., /) instead of backward slashes (i.e., \\). You should also select the edition of the SCF that you want to analyze inside load(). The format of the datasets' names is "scfYYYY.rda", where YYYY are the four digits of the SCF dataset edition. Changing these four digits for any of the editions downloaded (1998, 2001, 2004, 2007, 2010, 2013, 2016, 2019, 2022) loads the corresponding edition. For example, load("scf2022.rda") would load the 2022 edition.

```{r}
setwd( "C:/Users/amarc/OneDrive/Escritorio/SCF" )
load( "scf2022.rda" )

library(mitools)
library(survey)
library(downloader)
library(foreign)
library(DescTools)
library(dplyr)
library(tidyr)
library(ggplot2)
library(car)
library(pander)

interest_vars <- function(df){
  df$income_percentile <- cut(df$income, unique(quantile(df$income, seq(0, 1, 0.01))), labels = FALSE)
  df$networth_percentile <- cut(df$networth, unique(quantile(df$networth, seq(0, 1, 0.01))), labels = FALSE)
  df$debt2inc <- ifelse(df$debt2inc==0, 0.1, df$debt2inc)
  df$debt2inc <- cut(df$debt2inc, unique(quantile(df$debt2inc, seq(0, 1, 0.333))), labels = FALSE)
  df$levratio <- ifelse(df$levratio==0, 0.1, df$levratio)
  df$levratio <- cut(df$levratio, unique(quantile(df$levratio, seq(0, 1, 0.333))), labels = FALSE)
  df$stock_grade= ifelse(df$nstock>52.34, 10, df$nstock/5.134)
  df$overall_diversification_grade= ifelse(is.na((df$stocks*df$stock_grade +   df$stmutf*10)/df$deq), 0, (df$stocks*df$stock_grade + df$stmutf*10)/df$deq)
  df$overtrading_grade= ifelse(df$ntrad>750, 0, (10-((df$ntrad-50)/70)))
  df$overtrading_grade= ifelse(df$overtrading_grade>10, 10, df$overtrading_grade)
  df$efficiency= (df$overall_diversification_grade*df$overtrading_grade)/10
  df$efficiency_binary= ifelse(df$efficiency<5, 0, 1)
  factor_vars <- c("agecl", "debt2inc", "famstruct", "finlit", "hhsex", "efficiency_binary", "housecl", "hstocks", "htrad", "icall", "idont", "ifinplan",  "ifinpro", "ifriendwork", "iinternet",  "imagznews", "imailadtv", "inccat", "internet", "iother", "iself", "ishopgrdl", "ishopmodr", "ishopnone", "levratio", "nwcat", "occat1", "race", "savres1", "nofinrisk")
  df[factor_vars] <- lapply(df[,factor_vars], factor)
  cbind(df, df$efficiency_binary, df$income_percentile, df$networth_percentile)
  
#I have defined below the subset of variables and observations to keep. The conditions an individual needs to have in order to be selected are at least one of the following three: has trated in the last year (htrad==1), has stocks (hstocks==1, currently participates in the stock market (deq>0)
  subset(df, htrad==1 | hstocks==1 | deq>0, select = c(agecl, deq, debt2inc, educ, efficiency_binary, famstruct, finlit, hhsex, housecl, hstocks, htrad, icall, idont, ifinplan, ifinpro, ifriendwork, iinternet, imagznews, imailadtv, income_percentile, internet, iother, iself, ishopgrdl, ishopmodr, ishopnone, levratio, nstocks, ntrad, networth_percentile, occat1, race, savres1, stmutf, stocks, nofinrisk, wgt, y1, yy1))
}

#I have defined the implicates according to the variables of interest of the model
imp1 <- interest_vars(imp1)
imp2 <- interest_vars(imp2)
imp3 <- interest_vars(imp3)
imp4 <- interest_vars(imp4)
imp5 <- interest_vars(imp5)

#Since it is necessary to have the same number of observations to compute the MIresult() (our model), I have selected only the observations (yy1) that can save (df$savres1==0) across all five implicates. The process below overlaps the observations and selects only the ones that can save across all implicates and does the same for the weight file (to preserve relative weights in the regression).
common_obs <- Reduce(intersect, list(imp1$yy1, imp2$yy1, imp3$yy1, imp4$yy1, imp5$yy1, rw$yy1))
imp1 <- imp1[imp1$yy1 %in% common_obs, ]
imp2 <- imp2[imp2$yy1 %in% common_obs, ]
imp3 <- imp3[imp3$yy1 %in% common_obs, ]
imp4 <- imp4[imp4$yy1 %in% common_obs, ]
imp5 <- imp5[imp5$yy1 %in% common_obs, ]
rw <- rw[rw$yy1 %in% common_obs, ]

# clear up RAM
gc()

# end of memory conservation step #


# turn off scientific notation in most output
options( scipen = 20 )

scf.svyttest<-function(formula, design ,...){

	# the MIcombine function runs differently than a normal svyglm() call
	m <- eval(bquote(MIcombine( with( design , svyglm(formula,family=gaussian()))) ) )

	rval<-list(statistic=coef(m)[2]/SE(m)[2],
			   parameter=m$df[2],		
			   estimate=coef(m)[2],
			   null.value=0,
			   alternative="two.sided",
			   method="Design-based t-test",
			   data.name=deparse(formula))
			   
	rval$p.value <- ( 1 - pf( ( rval$statistic )^2 , 1 , m$df[2] ) )

	names(rval$statistic)<-"t"
	names(rval$parameter)<-"df"
	names(rval$estimate)<-"difference in mean"
	names(rval$null.value)<-"difference in mean"
	class(rval)<-"htest"

	return(rval)
  
}


# MIcombine() variant (code from the `mitools` package) that only uses
# the sampling variance from the *first* imputation instead of averaging all five
scf.MIcombine <-
	function (results, variances, call = sys.call(), df.complete = Inf, ...) {
		m <- length(results)
		oldcall <- attr(results, "call")
		if (missing(variances)) {
			variances <- suppressWarnings(lapply(results, vcov))
			results <- lapply(results, coef)
		}
		vbar <- variances[[1]]
		cbar <- results[[1]]
		for (i in 2:m) {
			cbar <- cbar + results[[i]]
			# MODIFICATION:
			# vbar <- vbar + variances[[i]]
		}
		cbar <- cbar/m
		# MODIFICATION:
		# vbar <- vbar/m
		evar <- var(do.call("rbind", results))
		r <- (1 + 1/m) * evar/vbar
		df <- (m - 1) * (1 + 1/r)^2
		if (is.matrix(df)) df <- diag(df)
		if (is.finite(df.complete)) {
			dfobs <- ((df.complete + 1)/(df.complete + 3)) * df.complete * 
			vbar/(vbar + evar)
			if (is.matrix(dfobs)) dfobs <- diag(dfobs)
			df <- 1/(1/dfobs + 1/df)
		}
		if (is.matrix(r)) r <- diag(r)
		rval <- list(coefficients = cbar, variance = vbar + evar * 
		(m + 1)/m, call = c(oldcall, call), nimp = m, df = df, 
		missinfo = (r + 2/(df + 3))/(r + 1))
		class(rval) <- "MIresult"
		rval
	}


# construct an imputed replicate-weighted survey design object
# build a new replicate-weighted survey design object,
# but unlike most replicate-weighted designs, this object includes the
# five multiply-imputed data tables - imp1 through imp5
scf.design <- 
	svrepdesign( 
		
		# use the main weight within each of the imp# objects
		weights = ~wgt , 
		
		# use the 999 replicate weights stored in the separate replicate weights file
		repweights = rw[ , -1 ] , 
		
		# read the data directly from the five implicates
		data = imputationList( list( imp1 , imp2 , imp3 , imp4 , imp5 ) ) , 

		scale = 1 ,

		rscales = rep( 1 / 998 , 999 ) ,

		# use the mean of the replicate statistics as the center
		# when calculating the variance, as opposed to the main weight's statistic
		mse = TRUE ,
		
		type = "other" ,

		combined.weights = TRUE
	)
```

The code below computes the model respecting the SCF design requirements. For an explanation of the variables' meaning check: <https://sda.berkeley.edu/sdaweb/docs/scfcomb2022/DOC/hcbkx01.htm>

```{r}
model_h2 <- MIcombine(
  with(scf.design, 
  svyglm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, family = quasibinomial)))

tStat <- model_h2$coefficients/sqrt(diag(model_h2$variance))
p_values <- round(2*pt(-abs(tStat),df=model_h2$df), 16)
significance <- add.significance.stars(p_values, cutoffs = c(0.05, 0.01, 0.001))
complete_2022_h2 <- cbind(summary(model_h2), tStat, p_values, significance)
complete_2022_h2
```

You can save the model in the folder you wish in order to avoid repeating the same computations:

```{r}
saveRDS(complete_2022_h2, file = "C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2022_h2.rds")
```

The code below checks if the data structure is adequate in order to comply with the assumptions of the logistic model. These characteristics are checked on each of the five implicates separately instead of the combined one as the functions required to do so do not work on MIresult objects (like the model estimated above). The assumptions taken when estimating a logit model are:

1.  Independence of observations: this is guaranteed by the design of the Survey of Consumer Finances. Not only are observations independent of each other but they are also weighted in accordance with the general US population in order not to get biased estimations. This assumption requires that we take into account the design of the SCF (its multiple implicates per observation), which we do above (see *scf.design*).

2.  Linearity in the logit for continuous variables. In addition to *income_percentile*, I have decided to treat *educ* as continuous since it is divided into many ordinal stages that hold a constant marginal relationship with the logarithm of the odds ratio of the dependent variable (*dummy_deq*: stock market participation).

```{r}
linear <- function(df){
  df <- na.omit(df)
  logistic_model <- glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data=df, family = binomial)
  probabilities <- predict(logistic_model, type = "response")
  predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
  mydata <- subset(df, select=-c(deq, nstocks, ntrad, stmutf, stocks, wgt, y1, yy1)) %>%
    dplyr::select_if(is.numeric) 
  predictors <- colnames(df)
  mydata <- mydata %>%
    mutate(logit = log(probabilities/(1-probabilities))) %>%
    gather(key = "predictors", value = "predictor.value", -logit)
  
  ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "gam") + 
  theme(aspect.ratio = 1) + 
  facet_wrap(~predictors, scales = "free_y")
}

linear(imp1)
linear(imp2)
linear(imp3)
linear(imp4)
linear(imp5)
```

3.  Lack of strongly influential outliers. Cook's distances over 1 represent influential outliers (R. Dennis Cook, 1982). Therefore, SCF datasets do not present influential outliers.

```{r}
cook_distance <- function(df){
  logistic_model <- glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data=df, family = binomial)
  cooksd <- cooks.distance(logistic_model)
  cooksd_out <- subset(cooksd, cooksd<(3*mean(cooksd)))
  plot(cooksd, pch = 20, main = "Cook's Distance Plot", ylab = "Cook's Distance", xlab = "Observation")
}

cook_distance(imp1)
cook_distance(imp2)
cook_distance(imp3)
cook_distance(imp4)
cook_distance(imp5)
```

4.  Absence of multicollinearity. VIF tests allow us to check for multicollinearity among variables. Values in the 0-5 range indicate low multicollinearity with other variables. The 5-10 range indicates moderate multicollinearity and values over 10 indicate high multicollinearity with other variables. The only concerning case in our dataset is the case of the *networth_percentile* variable, which usually lies just above the 5 threshold. However, since it is relatively low in the 5-10 range and the variable is highly significant at all levels considered, I have deliberately decided to keep it in spite of its moderately high multicollinearity among other variables.

```{r}
vif_check <- function(df){
  logistic_model <- glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = df, family = binomial)
  vif_values <- car::vif(logistic_model)
  print(vif_values)
}

vif_check(imp1)
vif_check(imp2)
vif_check(imp3)
vif_check(imp4)
vif_check(imp5)
```

I have also computed a diagnostic test using the Pseudo-R-squared by McFadden. Values between 0.2 and 0.4 are acceptable. Values above 0.4 indicate great goodness of fit (McFadden, 1977).

```{r}
PseudoR2(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp1, family = binomial), which = "McFadden")
PseudoR2(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp2, family = binomial), which = "McFadden")
PseudoR2(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp3, family = binomial), which = "McFadden")
PseudoR2(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp4, family = binomial), which = "McFadden")
PseudoR2(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp5, family = binomial), which = "McFadden")
```

This model, in contrast with the ones in hypothesis one, has a low Pseudo-R-squared by McFadden. This fact reiterates Campbell's (2006) complaint about the SCF being limited for assessing investing efficiency. The models devised to test the second hypothesis, despite complying with all four logistic model assumptions accurately predicts a few more cases than what would be expected with the null model (a model with only an intercept alone and no other variables). You can check below this computation for the year loaded. The first number is the number of cases predicted in excess of the null model and the second one is the percentage of extra correct predictions that this number represents (e.g. 177 and 16.97% respectively for the first implicate of the 2022 SCF edition):

```{r}
fitted_efficiency = ifelse(fitted(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp1, family = binomial))>0.5, 1, 0)

(sum(ifelse(fitted_efficiency==(na.omit(imp1)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp1$efficiency_binary==0)))

(sum(ifelse(fitted_efficiency==(na.omit(imp1)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp1$efficiency_binary==0)))/sum(na.omit(imp1$efficiency_binary==0))

fitted_efficiency = ifelse(fitted(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp2, family = binomial))>0.5, 1, 0)

(sum(ifelse(fitted_efficiency==(na.omit(imp2)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp2$efficiency_binary==0)))

(sum(ifelse(fitted_efficiency==(na.omit(imp2)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp2$efficiency_binary==0)))/sum(na.omit(imp2$efficiency_binary==0))

fitted_efficiency = ifelse(fitted(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp3, family = binomial))>0.5, 1, 0)

(sum(ifelse(fitted_efficiency==(na.omit(imp3)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp3$efficiency_binary==0)))

(sum(ifelse(fitted_efficiency==(na.omit(imp3)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp3$efficiency_binary==0)))/sum(na.omit(imp3$efficiency_binary==0))

fitted_efficiency = ifelse(fitted(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp4, family = binomial))>0.5, 1, 0)

(sum(ifelse(fitted_efficiency==(na.omit(imp4)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp4$efficiency_binary==0)))

(sum(ifelse(fitted_efficiency==(na.omit(imp4)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp4$efficiency_binary==0)))/sum(na.omit(imp4$efficiency_binary==0))

fitted_efficiency = ifelse(fitted(glm(efficiency_binary ~ agecl + debt2inc + educ + famstruct + hhsex + housecl + icall + ifinplan + ifinpro + ifriendwork + income_percentile + iinternet + imagznews + imailadtv + internet + iself + ishopgrdl + ishopmodr + levratio + networth_percentile + nofinrisk + occat1 + race, data = imp5, family = binomial))>0.5, 1, 0)

(sum(ifelse(fitted_efficiency==(na.omit(imp5)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp5$efficiency_binary==0)))

(sum(ifelse(fitted_efficiency==(na.omit(imp5)$efficiency_binary), 1, 0)==1)-sum(na.omit(imp5$efficiency_binary==0)))/sum(na.omit(imp5$efficiency_binary==0))
```

```{r}
complete_1998_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_1998_h2.rds")
complete_2001_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2001_h2.rds")
complete_2004_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2004_h2.rds")
complete_2007_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2007_h2.rds")
complete_2010_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2010_h2.rds")
complete_2013_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2013_h2.rds")
complete_2016_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2016_h2.rds")
complete_2019_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2019_h2.rds")
complete_2022_h2 <- readRDS("C:/Users/amarc/OneDrive/Escritorio/SCF Results/h2/complete_2022_h2.rds")
```

```{r}
financial_planner_complete = rbind(exp(complete_1998_h2["ifinplan1", 1]),
exp(complete_2001_h2["ifinplan1", 1]),
exp(complete_2004_h2["ifinplan1", 1]),
exp(complete_2007_h2["ifinplan1", 1]),
exp(complete_2010_h2["ifinplan1", 1]),
exp(complete_2013_h2["ifinplan1", 1]),
exp(complete_2016_h2["ifinplan1", 1]),
exp(complete_2019_h2["ifinplan1", 1]),
exp(complete_2022_h2["ifinplan1", 1]))

significance_financial_planner_complete= rbind((complete_1998_h2["ifinplan1", 8]),
(complete_2001_h2["ifinplan1", 8]),
(complete_2004_h2["ifinplan1", 8]),
(complete_2007_h2["ifinplan1", 8]),
(complete_2010_h2["ifinplan1", 8]),
(complete_2013_h2["ifinplan1", 8]),
(complete_2016_h2["ifinplan1", 8]),
(complete_2019_h2["ifinplan1", 8]),
(complete_2022_h2["ifinplan1", 8]))

networth_complete = rbind(exp(complete_1998_h2["networth_percentile", 1]),
exp(complete_2001_h2["networth_percentile", 1]),
exp(complete_2004_h2["networth_percentile", 1]),
exp(complete_2007_h2["networth_percentile", 1]),
exp(complete_2010_h2["networth_percentile", 1]),
exp(complete_2013_h2["networth_percentile", 1]),
exp(complete_2016_h2["networth_percentile", 1]),
exp(complete_2019_h2["networth_percentile", 1]),
exp(complete_2022_h2["networth_percentile", 1]))

significance_networth_complete = rbind((complete_1998_h2["networth_percentile", 8]),
(complete_2001_h2["networth_percentile", 8]),
(complete_2004_h2["networth_percentile", 8]),
(complete_2007_h2["networth_percentile", 8]),
(complete_2010_h2["networth_percentile", 8]),
(complete_2013_h2["networth_percentile", 8]),
(complete_2016_h2["networth_percentile", 8]),
(complete_2019_h2["networth_percentile", 8]),
(complete_2022_h2["networth_percentile", 8]))

income_complete = rbind(exp(complete_1998_h2["income_percentile", 1]),
exp(complete_2001_h2["income_percentile", 1]),
exp(complete_2004_h2["income_percentile", 1]),
exp(complete_2007_h2["income_percentile", 1]),
exp(complete_2010_h2["income_percentile", 1]),
exp(complete_2013_h2["income_percentile", 1]),
exp(complete_2016_h2["income_percentile", 1]),
exp(complete_2019_h2["income_percentile", 1]),
exp(complete_2022_h2["income_percentile", 1]))

significance_income_complete = rbind((complete_1998_h2["income_percentile", 8]),
(complete_2001_h2["income_percentile", 8]),
(complete_2004_h2["income_percentile", 8]),
(complete_2007_h2["income_percentile", 8]),
(complete_2010_h2["income_percentile", 8]),
(complete_2013_h2["income_percentile", 8]),
(complete_2016_h2["income_percentile", 8]),
(complete_2019_h2["income_percentile", 8]),
(complete_2022_h2["income_percentile", 8]))

coef_trends <- data.frame(Time = c(1998, 2001, 2004, 2007, 2010, 2013, 2016, 2019, 2022), "financial_planner_complete" = financial_planner_complete, "significance_financial_planner_complete" = significance_financial_planner_complete, "income_complete" = income_complete, "significance_income_complete" = significance_income_complete, "income_complete" = income_complete, "significance_income_complete" = significance_income_complete)
```

```{r}
ggplot(coef_trends, aes(x = Time)) +
  geom_line(aes(y = financial_planner_complete)) +
  geom_text(data=coef_trends, aes(x=Time, y=financial_planner_complete, label=significance_financial_planner_complete), vjust = 0) +
  labs(title = "Effect of Financial Planners on Investing Efficiency",
       x = "Year",
       y = "Odds Ratios")

ggplot(coef_trends, aes(x = Time)) +
  geom_line(aes(y = networth_complete)) +
  geom_text(data= coef_trends, aes(x=Time, y= networth_complete, label=significance_networth_complete), vjust = 0) +
  labs(title = "Effect of Net Worth Percentile Increases on Investing Efficiency",
       x = "Year",
       y = "Odds Ratios")

ggplot(coef_trends, aes(x = Time)) +
  geom_line(aes(y = income_complete)) +
  geom_text(data= coef_trends, aes(x=Time, y= income_complete, label=significance_income_complete), vjust = 0) +
  labs(title = "Effect of Income Percentile Increases on Investing Efficiency",
       x = "Year",
       y = "Odds Ratios")
```
